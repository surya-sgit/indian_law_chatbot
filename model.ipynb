{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10863926,"sourceType":"datasetVersion","datasetId":6749076},{"sourceId":10905122,"sourceType":"datasetVersion","datasetId":6777993}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade transformers accelerate bitsandbytes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ndevice = torch.device('cuda:0')\n\ntokenizer = AutoTokenizer.from_pretrained(\"unsloth/DeepSeek-R1-Distill-Qwen-7B-unsloth-bnb-4bit\")\nmodel = AutoModelForCausalLM.from_pretrained(\"unsloth/DeepSeek-R1-Distill-Qwen-7B-unsloth-bnb-4bit\").to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"filepath = '/kaggle/input/extracted-text/extracted_text.txt'\n\nwith open(filepath, 'r', encoding = 'utf-8') as file:\n    text = file.read()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\nchunks = text_splitter.split_text(text)\n\nprint(f\"Total Chunks: {len(chunks)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\nembedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\nvectors = embedder.encode(chunks)\n\nprint(f\"Embedding Shape: {vectors.shape}\")  # (num_chunks, embedding_size)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"jsonObj = pd.read_json(path_or_buf=\"/kaggle/input/trainh/train.jsonl\", lines=True)\njsonObj.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range (0, len(jsonObj['text'])):\n    (jsonObj['labels'][i].extend(jsonObj['text'][i]))\n\nprint('Completed')\n\njsonObj.drop(['text'], axis = 1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install chromadb","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import chromadb\n\n# Initialize ChromaDB\nchroma_client = chromadb.PersistentClient(path=\"/kaggle/working/chroma_db\")\n\ntext_collection = chroma_client.get_or_create_collection(name=\"text_collection\")\n\n# Add data\ntext_collection.add(\n    ids=[str(i) for i in range(len(chunks))],\n    documents=chunks,\n    embeddings=vectors.tolist()\n)\n\nprint(\"Stored in ChromaDB successfully!\")\n# Add data\njson_ids = [str(i) for i in jsonObj[\"id\"].tolist()]\njson_labels = jsonObj[\"labels\"].tolist()\n\njson_collection = chroma_client.get_or_create_collection(name=\"json_collection\")\njson_embeddings = embedder.encode(jsonObj[\"labels\"].tolist())\n\n# print(\"Sample JSON Labels Before Storage:\", json_labels[0])\n# print(\"Sample JSON IDs Before Storage:\", json_ids[0])\n# print(\"Sample JSON Embeddings Before Storage:\", json_embeddings[0])\n\n\nBATCH_SIZE = 40000  \n\nfor i in range(0, len(json_ids), BATCH_SIZE):\n    batch_ids = json_ids[i:i+BATCH_SIZE]\n    batch_embeddings = json_embeddings[i:i+BATCH_SIZE]\n    batch_documents = [str(doc) if doc is not None else \"Empty Document\" for doc in json_labels[i:i+BATCH_SIZE]]\n    \n    json_collection.add(\n        ids=batch_ids,\n        documents = batch_documents,\n        embeddings=batch_embeddings,\n           )\n\nprint(\"Stored JSON data in ChromaDB successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def retrieve_similar_text(query, k=3):\n    query_vector = embedder.encode([query]).tolist()\n    text_results = text_collection.query(query_embeddings=query_vector, n_results=k)\n    retrieved_texts = text_results.get(\"documents\", [[]])[0] if text_results.get(\"documents\") else []\n    # print(\"JSON Collection Count:\", json_collection.count())\n    json_results = json_collection.query(query_embeddings=query_vector, n_results=k, include = ['documents'])\n    # print(\"Full JSON Query Results:\", json_results)\n    # retrieved_jsons = json_results.get(\"documents\", [[]])[0] if json_results.get(\"documents\") else []\n    \n    retrieved_jsons = json_results.get(\"documents\", [])\n    retrieved_jsons = retrieved_jsons[0] if retrieved_jsons else []\n    \n    # print(\"retrieved_texts:\", retrieved_texts)\n    # print(\"retrieved_jsons:\", retrieved_jsons)\n\n    retrieved_texts = [text for text in retrieved_texts if isinstance(text, str)]\n    retrieved_jsons = [json_text for json_text in retrieved_jsons if isinstance(json_text, str)]\n    \n    retrieved_context = \"\\n\".join(retrieved_texts + retrieved_jsons)\n    \n    return retrieved_context if retrieved_context else \"No relevant context found.\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\ndef generate_response(query):  # lambda_value controls retrieval vs generation\n    retrieved_text = retrieve_similar_text(query, k=3)  # Get relevant text\n\n    if not retrieved_text or retrieved_text == \"No relevant documents found.\":\n        return \"I couldn't find relevant information.\"\n\n    print('goyal 1', retrieved_text)\n    \n    # Generate a retrieval-augmented response\n    retrieval_prompt = f'''You are an expert lawyer!!\n    This is the given context, use to this to summarize the condtion and explain all charges which will be applicable on such scenario and summarize how the proceedings will take place and required proofs which is acceptable in indian courts.\n    Context:\\n{retrieved_text}\\n\\n\n    Query: {query} Understand the query as a professional, respond as if you are directly talikng to victim or witness and help them how to fight agaisnt the crime that happened with them and summarise everything in 800 tokens\\n Answer:'''\n    retrieval_inputs = tokenizer(retrieval_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n    retrieval_output = model.generate(**retrieval_inputs , max_new_tokens=1000)\n    retrieval_response = tokenizer.decode(retrieval_output[0], skip_special_tokens=True)\n\n    if \"Answer:\" in retrieval_response:\n        retrieval_response = retrieval_response.split(\"Answer:\")[-1].strip()\n\n\n    print('goyal 3')\n    # Weighted blending of retrieval vs. model generation\n    final_response = f\"[Retrieved Info: {retrieval_response}]\\n\"\n    return final_response\n\n# Example usage:\nquery = \"What if some lady molested me?\"\nlambda_value = 1  # Higher = more reliance on retrieved text\nresponse = generate_response(query)\nprint(f\"Generated Response:\\n\", response)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ngrok config add-authtoken 2uA5CfIpTqIRSQUxvW19JQOf1eK_759BzJoR9iWLuX66AbN4e","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install fastapi uvicorn pyngrok torch\nfrom fastapi import FastAPI\nimport torch\nimport uvicorn\nfrom pyngrok import ngrok\n\n# Load your model\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model = torch.load(\"model.pth\", map_location=device)  # Replace with your model\n# model.eval()\n\n# Create API\napp = FastAPI()\n\n@app.post(\"/predict/\")\nasync def predict(query: str):\n    response = generate_response(query)\n    print(query)\n    return {\"prediction\": response}\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Expose API using ngrok\nngrok_tunnel = ngrok.connect(8000)\nprint(\"Public API URL:\", ngrok_tunnel.public_url)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nest_asyncio  # Fix for event loop issue\nimport threading\nimport asyncio\n\n# Apply nest_asyncio fix\nnest_asyncio.apply()\nasyncio.set_event_loop_policy(asyncio.DefaultEventLoopPolicy())\n\n\nconfig = uvicorn.Config(app, host=\"0.0.0.0\", port=8000)\nserver = uvicorn.Server(config)\n\nloop = asyncio.get_event_loop()\nloop.create_task(server.serve())  # âœ… This avoids blocking the loop","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null}]}